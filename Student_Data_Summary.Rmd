---
title: "IST 707 Final Project - Student Educational Outcomes"
author: "Christoper Murphy, Padmaja Kurumaddali, La Monte Yarroll"
date: "September 07, 2024"
output: word_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r include=FALSE}
#first, install and load required libraries
#using function provided by Professor Gary Krudys.

#specify the packages needed
packages=c("tidyverse", "ggplot2", "mclust", "rpart", "rpart.plot", "caret", "arules", "arulesViz", "corrplot", "Hmisc", "cluster", "knitr", "factoextra", "randomForest", "e1071", "pheatmap","pROC","reshape2","stats","nnet")

#use this function to check if each package is on the local machine
#if a package is installed, it will be loaded
#if any are not, the missing package(s) will be installed and loaded
package.check <- lapply(packages, FUN = function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, dependencies = TRUE)
    library(x, character.only = TRUE)
  }
})

#verify they are loaded
search()
```


```{r eval=FALSE, include=FALSE}
#References used to inform code choices:

```


# Introduction
 

# Analysis and Models


## About the data


### Dataset Summary
The following dataset was located on Kaggle at the following page:

https://www.kaggle.com/code/mdismielhossenabir/student-s-dropout-and-academic-success

The original source at UCI is here:

https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success

This dataset contains information on students' marital status, application details, previous qualifications, family background, admission grades, financial status, and academic performance. The target variable indicates whether a student dropped out or achieved academic success. The data spans multiple semesters and includes economic indicators such as unemployment rate, inflation rate, and GDP. Researchers can leverage this dataset to develop models for predicting dropout and academic success, contributing to the understanding of factors influencing educational outcomes.


#### Data Retrieval
The data is read in from the .CSV file.

```{r cache=TRUE, include=FALSE, label='get-data'}
filePath = "studentdata.csv"
studentSuccessData = read.csv2(filePath, stringsAsFactors = FALSE, dec=".")
str(studentSuccessData)
```


```{r cache=TRUE, include=FALSE, label='view-data-head'}
head(studentSuccessData, 5)
```


#### Data Attribute Summary

The Kaggle data card can also be found on the following page
https://www.kaggle.com/code/mdismielhossenabir/student-s-dropout-and-academic-success/input

#### Attribute Descriptions:
* Marital Status: The marital status of the student
* Application mode: The mode through which the student applied for admission
* Application order: The order in which the student applied for admission.
* Course: The academic course or program the student is enrolled in.
* Daytime/evening attendance: The attendance mode of the student (e.g., Daytime, Evening).
* Previous qualification: The highest academic qualification the student attained before the current enrollment.
* Previous qualification (grade): The grade or score achieved in the previous qualification.
* Nacionality: The nationality of the student.
* Mother's qualification: The highest academic qualification of the student's mother.
* Father's qualification: The highest academic qualification of the student's father.
* Mother's occupation: The occupation of the student's mother.
* Father's occupation: The occupation of the student's father.
* Admission grade: The grade or score obtained during the admission process.
* Displaced: Indicates whether the student has been displaced from their previous location.
* Educational special needs: Flags if the student has any educational special needs.
* Debtor: Indicates if the student is a debtor.
* Tuition fees up to date: Indicates whether the student's tuition fees are up to date.
* Gender: The gender of the student.
* Scholarship holder: Flags if the student is a holder of a scholarship.
* Age at enrollment: The age of the student at the time of enrollment.
* International: Indicates if the student is an international student.
* Curricular units 1st sem (credited): The number of curricular units credited in the first semester.
* Curricular units 1st sem (enrolled): The number of curricular units enrolled in the first semester.
* Curricular units 1st sem (evaluations): The number of evaluations conducted in the first semester.
* Curricular units 1st sem (approved): The number of curricular units approved in the first semester.
* Curricular units 1st sem (grade): The overall grade for curricular units in the first semester.
* Curricular units 1st sem (without evaluations): Curricular units in the first semester without evaluations.
* Curricular units 2nd sem (credited): The number of curricular units credited in the second semester.
* Curricular units 2nd sem (enrolled): The number of curricular units enrolled in the second semester.
* Curricular units 2nd sem (evaluations): The number of evaluations conducted in the second semester.
* Curricular units 2nd sem (approved): The number of curricular units approved in the second semester.
* Curricular units 2nd sem (grade): The overall grade for curricular units in the second semester.
* Curricular units 2nd sem (without evaluations): Curricular units in the second semester without evaluations.
* Unemployment rate: The unemployment rate at the time of data collection.
* Inflation rate: The inflation rate at the time of data collection.
* GDP: The Gross Domestic Product at the time of data collection.
* Target: The target variable indicating student outcome (e.g., Dropout, Academic Success).



\pagebreak

## Data Transformations

First, missing values are filled in with the mean for numeric columns, and mode for categorical columns.  Second, categorical columns are converted to type Factor. 

The structure of the data frame after transformation:

```{r cache=TRUE, include=FALSE, label='view-data-colsums'}
colSums(is.na(studentSuccessData))
```


```{r cache=TRUE, include=FALSE, label='remove-missing-data'}
# Perform necessary transformations

# Fill missing values with the mean for numeric columns and mode for categorical columns

studentSuccessData <- studentSuccessData %>%

  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .))) %>%

  mutate(across(where(is.character), ~ ifelse(is.na(.), Mode(.), .)))
 
# Function to calculate mode

Mode <- function(x) {

  ux <- unique(x)

  ux[which.max(tabulate(match(x, ux)))]

}
```



```{r cache=TRUE, include=FALSE, label='convert-attributes-to-factors'}

# Convert categorical columns to factor type

studentSuccessData <- studentSuccessData %>%

  mutate(

    Marital.status = factor(
      Marital.status,
      levels = c(1, 2, 3, 4, 5, 6),
      labels = c("Single", "Married", "Widower", "Divorced", "Facto Union", "Legally Separated")),

    Application.mode = factor(
      Application.mode,
      levels = c(1, 2, 5, 7, 10, 15, 16, 17, 18, 26, 27, 39, 42, 43, 44, 51, 53, 57),
      labels = c(
        "1st phase - general contingent",
        "Ordinance No. 612/93",
        "1st phase - special contingent (Azores Island)",
        "Holders of other higher courses",
        "Ordinance No. 854-B/99",
        "International student (bachelor)",
        "1st phase - special contingent (Madeira Island)",
        "2nd phase - general contingent",
        "3rd phase - general contingent",
        "Ordinance No. 533-A/99, item b2) (Different Plan)",
        "Ordinance No. 533-A/99, item b3 (Other Institution)",
        "Over 23 years old",
        "Transfer",
        "Change of course",
        "Technological specialization diploma holders",
        "Change of institution/course",
        "Short cycle diploma holders",
        "Change of institution/course (International)")),

    Application.order = factor(Application.order, ordered=TRUE, levels = c(1, 2, 3, 4, 5, 6), labels = c("1st", "2nd", "3rd", "4th", "5th", "6th")),
    
    Target = factor(Target),

    Course = factor(Course, levels = c(33,171,8014,9003,9070,9085,9119,9130,9147,9238,9254,9500,9556,9670,9773,9853,9991), labels = c(
      "Biofuel Production Technologies",
      "Animation and Multimedia Design",
      "Social Service (evening attendance) ",
      "Agronomy",
      "Communication Design",
      "Veterinary Nursing",
      "Informatics Engineering",
      "Equinculture",
      "Management",
      "Social Service",
      "Tourism",
      "Nursing",
      "Oral Hygiene",
      "Advertising and Marketing Management",
      "Journalism and Communication",
      "Basic Education",
      "Management (evening attendance)")),


    Previous.qualification = factor(
      Previous.qualification,
      levels = c(1, 2, 3, 4, 5, 6, 9, 10, 12, 14, 15, 19, 38, 39, 40, 42, 43),
      labels = c(
        "Secondary education",
        "Higher education - bachelor's degree",
        "Higher education - degree",
        "Higher education - master's",
        "Higher education - doctorate",
        "Frequency of higher education",
        "12th year of schooling - not completed",
        "11th year of schooling - not completed",
        "Other - 11th year of schooling",
        "10th year of schooling",
        "10th year of schooling - not completed",
        "Basic education 3rd cycle (9th/10th/11th year) or equiv.",
        "Basic education 2nd cycle (6th/7th/8th year) or equiv.",
        "Technological specialization course",
        "Higher education - degree (1st cycle)",
        "Professional higher technical course",
        "Higher education - master (2nd cycle)")),

    Nacionality = factor(
      Nacionality,
      levels = c(1, 2, 6, 11, 13, 14, 17, 21, 22, 24, 25, 26, 32, 41, 62, 100, 101, 103, 105, 108, 109),
      labels = c(
        "Portuguese",
        "German",
        "Spanish",
        "Italian",
        "Dutch",
        "English",
        "Lithuanian",
        "Angolan",
        "Cape Verdean",
        "Guinean",
        "Mozambican",
        "Santomean",
        "Turkish",
        "Brazilian",
        "Romanian",
        "Moldova (Republic of)",
        "Mexican",
        "Ukrainian",
        "Russian",
        "Cuban",
        "Colombian")),

    # Causes the error.
#     Mother.s.qualification <- ordered(Mother.s.qualification, levels = c(1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 14, 18, 19, 22, 26, 27, 29, 30, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44), labels = c(
#   "Secondary Education - 12th Year of Schooling or Eq.",
#   "Higher Education - Bachelor's Degree",
#   "Higher Education - Degree",
#   "Higher Education - Master's",
#   "Higher Education - Doctorate",
#   "Frequency of Higher Education",
#   "12th Year of Schooling - Not Completed",
#   "11th Year of Schooling - Not Completed",
#   "7th Year (Old)",
#   "Other - 11th Year of Schooling",
#   "10th Year of Schooling",
#   "General commerce course",
#   "Basic Education 3rd Cycle (9th/10th/11th Year) or Equiv.",
#   "Technical-professional course",
#   "7th year of schooling",
#   "2nd cycle of the general high school course",
#   "9th Year of Schooling - Not Completed",
#   "8th year of schooling",
#   "Unknown",
#   "Can't read or write",
#   "Can read without having a 4th year of schooling",
#   "Basic education 1st cycle (4th/5th year) or equiv.",
#   "Basic Education 2nd Cycle (6th/7th/8th Year) or Equiv.",
#   "Technological specialization course",
#   "Higher education - degree (1st cycle)",
#   "Specialized higher studies course",
#   "Professional higher technical course",
#   "Higher Education - Master (2nd cycle)",
#   "Higher Education - Doctorate (3rd cycle)"
# )),
# 
#  Father.s.qualification <- ordered(Father.s.qualification, levels = c(1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14, 18, 19, 20, 22, 25, 26, 27, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44), labels = c(
#   "Secondary Education - 12th Year of Schooling or Eq.",
#   "Higher Education - Bachelor's Degree",
#   "Higher Education - Degree",
#   "Higher Education - Master's",
#   "Higher Education - Doctorate",
#   "Frequency of Higher Education",
#   "12th Year of Schooling - Not Completed",
#   "11th Year of Schooling - Not Completed",
#   "7th Year (Old)",
#   "Other - 11th Year of Schooling",
#   "2nd year complementary high school course",
#   "10th Year of Schooling",
#   "General commerce course",
#   "Basic Education 3rd Cycle (9th/10th/11th Year) or Equiv.",
#   "Complementary High School Course",
#   "Technical-professional course",
#   "Complementary High School Course - not concluded",
#   "7th year of schooling",
#   "2nd cycle of the general high school course",
#   "9th Year of Schooling - Not Completed",
#   "8th year of schooling",
#   "General Course of Administration and Commerce",
#   "Supplementary Accounting and Administration",
#   "Unknown",
#   "Can't read or write",
#   "Can read without having a 4th year of schooling",
#   "Basic education 1st cycle (4th/5th year) or equiv.",
#   "Basic Education 2nd Cycle (6th/7th/8th Year) or Equiv.",
#   "Technological specialization course",
#   "Higher education - degree (1st cycle)",
#   "Specialized higher studies course",
#   "Professional higher technical course",
#   "Higher Education - Master (2nd cycle)",
#   "Higher Education - Doctorate (3rd cycle)"
# )),
# 
# Mother.s.occupation <- ordered(Mother.s.occupation, levels = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 90, 99, 122, 123, 125, 131, 132, 134, 141, 143, 144, 151, 152, 153, 171, 173, 175, 191, 192, 193, 194), labels = c(
#   "Student",
#   "Representatives of the Legislative Power and Executive Bodies, Directors, Directors and Executive Managers",
#   "Specialists in Intellectual and Scientific Activities",
#   "Intermediate Level Technicians and Professions",
#   "Administrative staff",
#   "Personal Services, Security and Safety Workers and Sellers",
#   "Farmers and Skilled Workers in Agriculture, Fisheries and Forestry",
#   "Skilled Workers in Industry, Construction and Craftsmen",
#   "Installation and Machine Operators and Assembly Workers",
#   "Unskilled Workers",
#   "Armed Forces Professions",
#   "Other Situation",
#   "(blank)",
#   "Health professionals",
#   "Teachers",
#   "Specialists in information and communication technologies (ICT)",
#   "Intermediate level science and engineering technicians and professions",
#   "Technicians and professionals, of intermediate level of health",
#   "Intermediate level technicians from legal, social, sports, cultural and similar services",
#   "Office workers, secretaries in general and data processing operators",
#   "Data, accounting, statistical, financial services and registry-related operators",
#   "Other administrative support staff",
#   "Personal service workers",
#   "Sellers",
#   "Personal care workers and the like",
#   "Skilled construction workers and the like, except electricians",
#   "Skilled workers in printing, precision instrument manufacturing, jewelers, artisans and the like",
#   "Workers in food processing, woodworking, clothing and other industries and crafts",
#   "Cleaning workers",
#   "Unskilled workers in agriculture, animal production, fisheries and forestry",
#   "Unskilled workers in extractive industry, construction, manufacturing and transport",
#   "Meal preparation assistants"
# )),
# 
#  Father.s.occupation <- factor(Father.s.occupation,
# levels = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 90, 99, 101, 102, 103, 112, 114, 121, 122, 123, 124, 131, 132, 134, 135, 141, 143, 144, 151, 152, 153, 154, 161, 163, 171, 172, 174, 175, 181, 182, 183, 192, 193, 194, 195), labels = c(
#   "Student",
#   "Representatives of the Legislative Power and Executive Bodies, Directors, Directors and Executive Managers",
#   "Specialists in Intellectual and Scientific Activities",
#   "Intermediate Level Technicians and Professions",
#   "Administrative staff",
#   "Personal Services, Security and Safety Workers and Sellers",
#   "Farmers and Skilled Workers in Agriculture, Fisheries and Forestry",
#   "Skilled Workers in Industry, Construction and Craftsmen",
#   "Installation and Machine Operators and Assembly Workers",
#   "Unskilled Workers",
#   "Armed Forces Professions",
#   "Other Situation",
#   "(blank)",
#   "Armed Forces Officers",
#   "Armed Forces Sergeants",
#   "Other Armed Forces personnel",
#   "Directors of administrative and commercial services",
#   "Hotel, catering, trade and other services directors",
#   "Specialists in the physical sciences, mathematics, engineering and related techniques",
#   "Health professionals",
#   "Teachers",
#   "Specialists in finance, accounting, administrative organization, public and commercial relations",
#   "Intermediate level science and engineering technicians and professions",
#   "Technicians and professionals, of intermediate level of health",
#   "Intermediate level technicians from legal, social, sports, cultural and similar services",
#   "Information and communication technology technicians",
#   "Office workers, secretaries in general and data processing operators",
#   "Data, accounting, statistical, financial services and registry-related operators",
#   "Other administrative support staff",
#   "Personal service workers",
#   "Sellers",
#   "Personal care workers and the like",
#   "Protection and security services personnel",
#   "Market-oriented farmers and skilled agricultural and animal production workers",
#   "Farmers, livestock keepers, fishermen, hunters and gatherers, subsistence",
#   "Skilled construction workers and the like, except electricians",
#   "Skilled workers in metallurgy, metalworking and similar",
#   "Skilled workers in electricity and electronics",
#   "Workers in food processing, woodworking, clothing and other industries and crafts",
#   "Fixed plant and machine operators",
#   "Assembly workers",
#   "Vehicle drivers and mobile equipment operators",
#   "Unskilled workers in agriculture, animal production, fisheries and forestry",
#   "Unskilled workers in extractive industry, construction, manufacturing and transport",
#   "Meal preparation assistants",
#   "Street vendors (except food) and street service providers"
# )),
# Displaced <- factor(Displaced, levels = c(1, 0), labels = c("yes", "no")),
# Educational.special.needs <- factor(Educational.special.needs, levels = c(1, 0), labels = c("yes", "no")),
# Debtor <- factor(Debtor, levels = c(1, 0), labels = c("yes", "no")),
# Tuition.fees.up.to.date <- factor(Tuition.fees.up.to.date, levels = c(1, 0), labels = c("yes", "no")),
# Gender <- factor(Gender, levels = c(1, 0), labels = c("yes", "no")),
# Scholarship.holder <- factor(Scholarship.holder, levels = c(1, 0), labels = c("yes", "no")),
# International <- factor(International, levels = c(1, 0), labels = c("yes", "no")),
    # Daytime.evening.attendance. <- factor(Daytime.evening.attendance., levels = c(1, 0), labels = c("daytime", "evening"))
)
```


```{r echo=FALSE, cache=TRUE, label='view-data-str'}
#validate structure after attribute transformation
str(studentSuccessData)
```



```{r cache=TRUE, include=FALSE, label='remove-complete-cases'}

# Remove rows with any NA values
studentSuccessData <- studentSuccessData[complete.cases(studentSuccessData), ]

```


The following summarizes the data after transformation:

```{r echo=FALSE, cache=TRUE, label='view-data-summary'}
summary(studentSuccessData)
```



\pagebreak
## Exploratory Data Analysis

To aid in training and testing prediction models, the data is divided into a reserved test data set (30% of the data) and a train data set (70% of the data).  A copy of the test and train datasets are created to support support vector machine training and testing.

```{r include=FALSE, label='create-test-train', cache=TRUE}
set.seed(711)
ind <- sample(c(TRUE, FALSE), size=nrow(studentSuccessData), replace=TRUE, prob=c(0.7, 0.3))
train <- studentSuccessData[ind, ]
test <- studentSuccessData[!ind, ]

# Capture the values as reference to compare in confusion matrices.
test.target <- test$Target
```


```{r cache=TRUE, include=FALSE, label='create-test-train-svm'}
#capture new data frames to isolate to SVM
svm_train <- train
svm_test <- test

# remove labels from the test set
svm_test_label <- svm_test$Target

svm_test <- svm_test %>% select(-Target)
```


The following plots describe the distributions of the attributes of the dataset.

```{r echo=FALSE, label='plot-all-attributes', cache=TRUE}
 
columns <- colnames(train)
# Loop through each column and create a plot
for (col in columns) {
  # Check if the column is numeric or categorical
  if (is.numeric(train[[col]])) {
    # Create a histogram for numeric columns
    p <- ggplot(train, aes_string(x = col)) +
      geom_histogram(binwidth = 1, fill = "blue", color = "black") +
      ggtitle(paste("Histogram of", col)) +
      theme_minimal()
  } else {
    # Create a bar plot for categorical columns
    p <- ggplot(train, aes_string(x = col)) +
      geom_bar(fill = "blue", color = "black") +
      ggtitle(paste("Bar Plot of", col)) +
      theme_minimal()
  }
  # Print the plot
  print(p)
}
```


```{r eval=FALSE, include=FALSE, label='plot-small-multiple', cache=TRUE}
#the following graph shows an example of a small multiple from the dataset.
#base_graph <- ggplot(train, aes(Target)) + geom_bar()

#base_graph + facet_grid(cols=vars(Marital.status), rows=vars(Nacionality))
```



\pagebreak
### Correllations

The following plots summarize the correlations between attributes.

```{r echo=FALSE, label='plot-correlations', cache=TRUE}
train_m <- sapply(train, as.numeric)
train_complete <- complete.cases(train_m)
train_m <- train_m[train_complete, ]
train.cor <- cor(train_m, method=c("pearson"))
corrplot(train.cor, tl.cex = 0.3)
heatmap(train.cor)
```

\pagebreak
### Bi-Variate EDA

#### Student Age at Enrollment and Target

The following plot explores the spread of the student age ranges at each Target level.

```{r echo=FALSE, label='plot-bivariate-eda', cache=TRUE}

# Plot for Age and Target

ggplot(train, aes(x = Age.at.enrollment, y = Target)) +
  geom_point() +
  geom_smooth(method = "lm", col = "blue") +
  labs(title = "Scatter Plot of Age.at.enrollment  vs Target",
       x = "Age.at.enrollment ",
       y = "Target") +
  theme_minimal()

```


#### Mother's Qualification and Target

The following plot explores the distribution of Mother's qualification at each Target level.

```{r echo=FALSE, label='plot-mothers-qualifications', cache=TRUE}
# Bar plot for Mother's Qualification and Target
ggplot(train, aes(x = factor(Mother.s.qualification), fill = factor(Target))) +
  geom_bar(position = "dodge") +
  labs(title = "Bar Plot of Mother's Qualification vs Target",
       x = "Mother's Qualification",
       fill = "Target") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set3") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


#### Father's Qualification and Target

The following plot explores the distribution of Father's qualification at each Target level.

```{r echo=FALSE , label='plot-fathers-qualifications', cache=TRUE}
# Bar plot for Father's Qualification and Target
ggplot(train, aes(x = factor(Father.s.qualification), fill = factor(Target))) +
  geom_bar(position = "dodge") +
  labs(title = "Bar Plot of Father's Qualification vs Target",
       x = "Father's Qualification",
       fill = "Target") +
  theme_minimal() +
  scale_fill_manual(values = c("#DC143C", "#1f77b4", "#2ca02c"))  

```


#### Mother's Occupation, Father's Occupation, and Target

The following plots explore the distributions of parental occupations at each Target level.

```{r echo=FALSE, label='plot-parents-occupations', cache=TRUE}
# Bar plot for Father's Occupation and Target
ggplot(train, aes(x = factor(Father.s.occupation), fill = factor(Target))) +
  geom_bar(position = "dodge") +
  labs(title = "Bar Plot of Father's Occupation vs Target",
       x = "Father's Occupation",
       fill = "Target") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set3") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Bar plot for Mother's Occupation and Target
ggplot(train, aes(x = factor(Mother.s.occupation), fill = factor(Target))) +
  geom_bar(position = "dodge") +
  labs(title = "Bar Plot of Mother's Occupation vs Target",
       x = "Mother's Occupation",
       fill = "Target") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set3") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


#### Previous Qualification and Target

The following plot explores the distribution of a student's previous qualification at each Target level.

```{r echo=FALSE, label='plot-previous-qualifications', cache=TRUE}
# Bar plot for Previous Qualification and Target
ggplot(train, aes(x = factor(Previous.qualification), fill = factor(Target))) +
  geom_bar(position = "dodge")  +
  labs(title = "Bar Plot of Previous Qualification vs Target",
       x = "Previous Qualification",
       fill = "Target") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set3") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```




\pagebreak
## Models

### Base Models

#### O Models: Random and Majority

The models calculated later all need to outperform the most naive models: random guessing, and guess the most popular.

The following shows the confusion matrix and accuracy for a random guess model: 


#### 0 Model: Random Guessing

```{r echo=FALSE, label='model-random-guess', cache=TRUE}
set.seed(1701)
randomGuessPrediction <- sample(train$Target, nrow(test), replace=FALSE)
confusionMatrix(randomGuessPrediction, test$Target)
```

Random guessing from a distribution that matches the training data gives an accuracy of about 38%. No model should underperform random guessing.



\pagebreak
#### 0 Model: Majority

The following shows the confusion matrix and accuracy for a most common value model: 

```{r echo=FALSE, label='model-majority', cache=TRUE}
graduate <- test$Target[test$Target == "Graduate"][1]
majorityModelPrediction <- rep(graduate, nrow(test))
confusionMatrix(majorityModelPrediction, test$Target)
```

Picking the most common value for every test record, yields an accuracy of 52%. Later models should beat 52% accuracy to be interesting.


### Association Rule Mining

For association rule mining, the data is converted into transactions, association rules are generated, and visualized.

```{r include=FALSE, label='model-asm', cache=TRUE}
# For association rule mining, we need to convert the data into transactions:
library(arules)

train.arm <- train

# Identify numeric columns
numeric_columns <- sapply(train, is.numeric)

# Discretize numeric columns
for (col in names(train)[numeric_columns]) {
  train.arm[[col]] <- discretize(train[[col]], method = "interval", breaks = 5)
}

transaction_id  <- as.character(train.arm[["transid"]])
train.arm$transid <- NULL
transactions <- as(train.arm, "transactions")

```


```{r include=FALSE, label='model-asm-rules', cache=TRUE}
# Generate association rules
rules_dropout <- apriori(transactions, parameter = list(supp = 0.01, conf = 0.5, target = "rules"),
                 appearance = list(rhs = c("Target=Dropout" ), default = "lhs") , control = list(verbose=F))
rules_graduate <- apriori(transactions, parameter = list(supp = 0.01, conf = 0.5, target = "rules"),
                 appearance = list(rhs = c("Target=Graduate" ), default = "lhs") , control = list(verbose=F))
```


```{r echo=FALSE, label='model-plot-asm-rules', cache=TRUE}

library(arulesViz)

# Inspect the top rules of Dropout by lift
rules_dropout_sorted <- sort(rules_dropout,decreasing = TRUE, by = "lift")
inspect(rules_dropout_sorted[1:10])

# Inspect the top rules of Dropout by confidence
rules_dropout_sorted <- sort(rules_dropout_sorted,decreasing = TRUE, by = "confidence")
inspect(rules_dropout_sorted[1:10])

#summarize the rules
summary(rules_dropout_sorted)
```


```{r eval=FALSE, include=FALSE, label='model-asm-graph-interactive', cache=TRUE}
#visualize
plot(rules_dropout_sorted[1:10], method="graph", engine = "interactive")
```



```{r echo=FALSE, label='model-asm-rules-graduate', cache=TRUE}

# Inspect Top rules of graduate

rules_graduate_sorted <- sort(rules_graduate ,decreasing = TRUE, by = "lift")
inspect(rules_graduate_sorted[1:10])
rules_graduate_sorted <- sort(rules_graduate_sorted,decreasing = TRUE, by = "confidence")
inspect(rules_graduate_sorted[1:10])
summary(rules_graduate_sorted)
plot(rules_graduate_sorted[1:10], method="graph", engine = "interactive")
```



```{r eval=FALSE, include=FALSE, label='clusters-by-target', cache=TRUE}
# Here are the clusters grouped by Target.  
# ggplot(data=emdata, aes(x=Target, fill=cluster)) +
#     geom_bar(stat="count") +
#     labs(title = "K = ?") +
#     theme(plot.title = element_text(hjust=0.5),  text=element_text(size=15))
```


\pagebreak
### Clustering Models

#### k-Means 

Seven clusters for k-Means produces a pleasing result with the following parameters.

```{r include=FALSE, label='model-kmeans', cache=TRUE}
train_km <- train_m
rownames(train_km) <- train[train_complete, ]$Target

set.seed(20)
```


```{r echo=FALSE, label='model-kmeans-clusters', cache=TRUE}
Clusters <- kmeans(train_km, 12, iter.max = 100, nstart = 7, algorithm="Hartigan-Wong")
```


```{r echo=FALSE, label='model-kmeans-plot1', cache=TRUE}
summary(Clusters)
Clusters$cluster[1:14]
```

```{r echo=FALSE, label='model-kmeans-plot2', cache=TRUE}
train_km2 <- train[train_complete, ]
train_km2$cluster <- as.factor(Clusters$cluster)
clusplot(train_km, train_km2$cluster, color=TRUE, shade=TRUE, labels=0, lines=0)
```


This dataset does not yield to k-Means clustering, at least with all the numeric variables.

```{r echo=FALSE, label='model-kmeans-plot3', cache=TRUE}
ggplot(data=train_km2, aes(x=Target, fill=cluster)) +
    geom_bar(stat="count") +
    labs(title = "K = 12") +
    theme(plot.title = element_text(hjust=0.5),  text=element_text(size=15))
```


\pagebreak
### Classification Models

#### Decision Tree Model


The following section shows the decision tree model, and reports the confusion matrix and accuracy results.

```{r cache=TRUE, include=FALSE, label='model-decision-tree'}

### For decision tree classification, we'll use the `rpart` package:

# We need to split the dataset, but since we already partitioned the dataset into train and test, we will directly use that for training our model.

# Train the decision tree model
tree_model <- rpart(Target ~ ., data = train, method = "class")
```



```{r cache=TRUE, include=FALSE, label='model-pruned-tree'}
# Prune the tree to simplify it
pruned_tree <- prune(tree_model, cp = tree_model$cptable[which.min(tree_model$cptable[,"xerror"]),"CP"])
```



```{r echo=FALSE, cache=TRUE, label='model-pruned-tree-plot'}
# Plot the pruned tree with improved readability
par(mar = c(5, 4, 4, 2) + 0.1)  # Adjust the margins as needed

rpart.plot(pruned_tree, 
           type = 2, 
           extra = 104, 
           fallen.leaves = TRUE, 
           cex = 0.8,  # Adjust text size
           box.palette = "RdYlGn", 
           shadow.col = "gray", 
           nn = TRUE,
           faclen = 0,  # Full names
           varlen = 0,  # Full variable names
           compress = TRUE)  # Compress the tree

```


```{r cache=TRUE, include=FALSE, label='model-decision-tree-predictions'}
#drop the orginal target value
test %>% select(-Target)


#create the predictions
predictions <- predict(tree_model, newdata=test, type = "class")

```


```{r cache=TRUE, echo=FALSE, label='model-decision-tree-confusionmatrix'}
# Confusion matrix
confusionMatrix(predictions, test.target)
```


\pagebreak
#### Support Vector Machine with Linear Kernel

A Support Vector machine with a linear kernel model is trained on the training data using 2-fold cross validation.  The model leverages an expanded grid to test for the following values of the C hyperparameter: 0.1, 1, 10. C is a regularization parameter that controls the balance between achieving a low error and minimizing the model complexity.  

The model also is configured to preprocess the training data to remove attributes with 0 variance, and apply centering across all of the samples.  This centers the predictors by subtracting the mean of each predictor from the data, standardizing the data so that the mean of each predictor is zero.

```{r cache=TRUE, include=FALSE, label='model-svm-linear'}
# Define train control for SVM
# Run with 2-fold cross validation
# Compute performance metrics for multiclass classification problem
# save predictions for the final model
train_control_svm_linear = trainControl(
  method = "cv",
  number = 2,  # 2-fold cross-validation
  summaryFunction = multiClassSummary,
  savePredictions = "final"
)

#create a dataframe to specify the values of the C hyper-parameter (cost) to try when tuning the model.
#C is a regularization parameter that controls the balance between achieving a low error and minimizing the model complexity.
#A higher C value means the model will fit the training data more closely.
#A lower C value means the model will be more general.
tune_grid = expand.grid(C = c(0.1, 1, 10))

# Train SVM model with linear kernel and preprocessing included directly in the train function
# Pre-process removes attributes with 0 variance apply centering across all samples
svm_linear_model = caret::train(
  Target ~ ., 
  data = svm_train, 
  method = "svmLinear",
  trControl = train_control_svm_linear,
  preProcess = c("zv","center"),  
  tuneGrid = tune_grid
)
```


The following shows the SVM linear model statistics for each value of the C hyper-parameter used to tune the model:

```{r echo=FALSE, cache=TRUE, label='model-svm-linear-stats'}
# View SVM linear model statistics
print(svm_linear_model)
```

The following plot shows the Accuracy of the SVM Linear model against the 3 cost hyperparameters.

```{r echo=FALSE, cache=TRUE, label='model-svm-linear-plot'}
# Plot the SMV Linear model performance
plot(svm_linear_model)
```


```{r cache=TRUE, include=FALSE, label='model-svm-linear-confusionmatrix-train'}
#The following reports the Confusion Matrix and Accuracy of the SVM Linear model with the train dataset and the C hyperparameter =0.1.

# View confusion matrix of out-of-fold predictions
svm_out_of_fold_preds = svm_linear_model$pred
svm_conf_matrix = confusionMatrix(svm_out_of_fold_preds$pred, svm_out_of_fold_preds$obs)
svm_conf_matrix
```

Finally, the model is tested using the test dataset to show the final accuracy:

```{r cache=TRUE, include=FALSE, label='model-svm-linear-test-predictions'}
#view the optimal parameters
# Make predictions on the new dataset using the trained SVM model
svm_linear_predictions <- predict(svm_linear_model, newdata = svm_test)
```


```{r echo=FALSE, cache=TRUE, label='model-svm-linear-test-confusionmatrix'}
# Calculate the confusion matrix
conf_matrix_svm_linear <- confusionMatrix(svm_linear_predictions, svm_test_label)

# Output the confusion matrix
print(conf_matrix_svm_linear)
```


\pagebreak
#### Support Vector Machine with Polynomial Kernel
Next, a Support Vector machine with a polynomial kernel model is trained on the training data using 2-fold cross validation.  The model is configured to leverage the following multiple hyperparameters: C, Degree, and Scale.  Building the model with multiple hyperparameters with multiple values is implemented to improve the accuracy of the model based on the optimal combination of hyperparameter values.  


The C hyperparameter is a regularization parameter that controls the balance between achieving a low error and minimizing the model complexity.  The following values of the C hyperparameter are used by the model: 0.1, 1, 10.

For polynomial kernel SVMs, degree determines the degree of the polynomial used in the kernel function, affects the complexity of the decision boundary, and controls the dimensionality of the feature space after transformation. A higher degree results in a higher-dimensional space.  The following values of the Degree hyperparameter are used by the model: 2, 3, 4.

A low Degree (e.g., 2): A lower degree polynomial kernel creates simpler decision boundaries. It captures simpler relationships between the features and the target variable. With a degree of 2, the polynomial kernel generates interaction terms and quadratic features. Degree 2: Adds quadratic terms and interactions, making the model capable of capturing more complex relationships than a linear model.  

A high Degree (e.g., 3, 4, or more) creates more complex decision boundaries, captures more intricate relationships and interactions between features, which helps with data that is more complex but also increases the risk of over-fitting. Degree 3 and Above: These degrees add even higher-order terms, which increases the model’s ability to fit intricate patterns in the training data.

The Scale value used is calculated as a ratio of the number of features in the dataset (0.0278).  If scale is a small value, it effectively makes the polynomial transformation more sensitive to variations in input features. This can lead to more complex decision boundaries. For this analysis, the scale property for the polynomial kernel is set as a constant.

The model also is configured to preprocess the training data to remove attributes with 0 variance, and apply centering across all of the samples.  This centers the predictors by subtracting the mean of each predictor from the data, standardizing the data so that the mean of each predictor is zero.

```{r include=FALSE, label='model-svm-poly', cache=TRUE}
# Define train control for SVM
# Run with 2-fold cross validation
# Compute performance metrics for multiclass classification problem
# save predictions for the final model

train_control_svm_poly = trainControl(
  method = "cv",
  number = 2,  # 2-fold cross-validation
  summaryFunction = multiClassSummary,
  savePredictions = "final"
)

# Calculate the number of features in the training data 
# excluding 1 data to account for the Target column
num_features = ncol(svm_train) - 1

# Calculate the scale value as 1 / number of features
scale_value = 1 / num_features

#create a data frame of all possible hyper parameter combinations for tuning the SVM with polynomial kernel.
tune_grid_svm_poly = expand.grid(C = c(0.1, 1, 10), degree = c(2, 3, 4), scale=scale_value)  # Only tuning C and degree, not scale


# Train SVM model with polynomial kernel and preprocessing included directly in the train function
svm_poly_model = caret::train(
  Target ~ ., 
  data = svm_train, 
  method = "svmPoly",  
  trControl = train_control_svm_poly,
  preProcess = c("zv", "center"),  # remove attributes with 0 variance across all samples, apply centering
  tuneGrid = tune_grid_svm_poly  
)
```


The following shows the SVM Polynomial model statistics for each combination hyperparameters used to tune the model (C and Degree, only, as Scale was set at a constant value):

```{r echo=FALSE, cache=TRUE, label='model-svm-poly-stats'}
# View model statistics
print(svm_poly_model)
```


The following plot shows the Accuracy of the SVM Polynomial model against the 3 cost and Degree hyperparameters.

```{r echo=FALSE, label='model-svm-poly-params', cache=TRUE}
# Plot model performance
plot(svm_poly_model)
```




```{r cache=TRUE, include=FALSE, label='model-svm-poly-confusionmatrix'}
#The following reports the confusion Matrix and Accuracy of the SVM Polynomial model during training with the highest accuracy (C=1.0, Degree=2).
# View confusion matrix of out-of-fold predictions
svm_poly_out_of_fold_preds = svm_poly_model$pred
svm_conf_matrix = confusionMatrix(svm_poly_out_of_fold_preds$pred, svm_poly_out_of_fold_preds$obs)
print(svm_conf_matrix)
```

The following reports the optimal hyperparameters reported by the SVM Polynomial model generation.

```{r echo=FALSE, label='model-svm-poly-tune', cache=TRUE}
#view the optimal parameters
svm_poly_model$bestTune
```

Finally, the model is tested using the test dataset to show the final accuracy:

```{r cache=TRUE, include=FALSE, label='model-svm-poly-test-predictions'}
#view the optimal parameters
# Make predictions on the new dataset using the trained SVM model
svm_poly_predictions <- predict(svm_poly_model, newdata = svm_test)
```


```{r echo=FALSE, cache=TRUE, label='model-svm-poly-test-confusionmatrix'}
# Calculate the confusion matrix
conf_matrix_svm_poly <- confusionMatrix(svm_poly_predictions, svm_test_label)

# Output the confusion matrix
print(conf_matrix_svm_poly)
```


\pagebreak
The following section leverages PCA to reduce the dimensions of the dataset to the attributes having the most significant impact on the outcome.

#### Dimensionality Reduction


```{r echo=FALSE, label='pca-visualization', cache=TRUE}
# Perform PCA
pca <- prcomp(train %>% select(where(is.numeric)), center = TRUE, scale. = TRUE)

# Summary of PCA
summary(pca)

# Append PCA results to the dataset
train_pca <- as.data.frame(pca$x)
train_pca$Target <- train$Target

# Visualize PCA results
ggplot(train_pca, aes(x = PC1, y = PC2, color = Target)) +
  geom_point() +
  labs(title = "PCA of Student Success Data",
       x = "Principal Component 1",
       y = "Principal Component 2") +
  theme_minimal()


# Visualize the explained variance
fviz_eig(pca, addlabels = TRUE, ylim = c(0, 50))

```


#### PCA on Selected Factors

Based on the Scree plot, it looks like the first few principal components explain a significant portion of the variance. Let’s decide to select the first two principal components for further analysis.

```{r echo=FALSE, label='pca-selected-factors', cache=TRUE}

# Select the number of principal components based on the Scree plot
selected_pcs <- 2

# Transform the data using the selected principal components
train_pca_selected <- as.data.frame(pca$x[, 1:selected_pcs])
train_pca_selected$Target <- train$Target

# Visualize the explained variance
fviz_eig(pca, addlabels = TRUE, ylim = c(0, 50))

# Visualize PCA results
ggplot(train_pca_selected, aes(x = PC1, y = PC2, color = Target)) +
  geom_point() +
  labs(title = "PCA of Student Success Data (Selected Components)",
       x = "Principal Component 1",
       y = "Principal Component 2") +
  theme_minimal()


```

#### Naive Bayes with K-fold CV on Non-PCA Data


```{r echo=FALSE, label='model-nb-kfold-no-pca', cache=TRUE}
# Set up k-fold cross-validation
kfolds <- 10
folds <- createFolds(train$Target, k = kfolds)

AllResults_non_pca <- list()
AllLabels_non_pca <- list()

for (k in 1:kfolds) {
  fold_train <- train[folds[[k]], ]
  fold_test <- train[-folds[[k]], ]
  
  # Train Naive Bayes Model
  naive_bayes_model <- naiveBayes(Target ~ ., data = fold_train)
  
  # Predict on Test Data
  nb_Pred <- predict(naive_bayes_model, newdata = fold_test)
  
  #Visualize
  plot(nb_Pred, ylab= "Density", main = "Naive Bayes Plot")
  
  # Ensure predictions and actual labels are factors with the same levels
  nb_Pred <- factor(nb_Pred, levels = unique(c(levels(nb_Pred), levels(fold_test$Target))))
  fold_test$Target <- factor(fold_test$Target, levels = unique(c(levels(nb_Pred), levels(fold_test$Target))))
  
  # Visualize Naive Bayes Predictions
  plot <- ggplot(data.frame(Prediction = nb_Pred), aes(x = Prediction)) +
    geom_density(fill = "blue", alpha = 0.5) +
    labs(title = paste("Naive Bayes Plot - Fold", k), y = "Density") +
    theme_minimal()
  print(plot)
  
  # Evaluate Model
  conf_matrix <- confusionMatrix(nb_Pred, fold_test$Target)
  print(conf_matrix)
  
  # Calculate accuracy
  accuracy <- sum(diag(conf_matrix$table)) / sum(conf_matrix$table)
  print(paste("Accuracy: Fold", k, "-", round(accuracy * 100, 2), "%", sep = " "))
  
  # Accumulate results from each fold
  AllResults_non_pca <- c(AllResults_non_pca, nb_Pred)
  AllLabels_non_pca <- c(AllLabels_non_pca, fold_test$Target)
}

# Confusion Matrix (across all folds)
final_conf_matrix_non_pca <- table(unlist(AllResults_non_pca), unlist(AllLabels_non_pca))
print(final_conf_matrix_non_pca)

# Calculate accuracy from the confusion matrix
nb_accuracy_non_pca <- sum(diag(final_conf_matrix_non_pca)) / sum(final_conf_matrix_non_pca)
print(paste("Overall Accuracy (Non-PCA): ", round(nb_accuracy_non_pca * 100, 2), "%", sep = ""))

# Plot Confusion Matrix as Heatmap
heatmap_data_non_pca <- as.matrix(final_conf_matrix_non_pca)
pheatmap(heatmap_data_non_pca, main = "Confusion Matrix Heatmap (Non-PCA)", color = colorRampPalette(c("white", "blue"))(50))

# Plotting ROC Curve
roc_curve_non_pca <- roc(unlist(AllLabels_non_pca), as.numeric(unlist(AllResults_non_pca)))
ggroc(roc_curve_non_pca) +
  ggtitle("ROC Curve (Non-PCA)") +
  xlab("1 - Specificity") +
  ylab("Sensitivity")



```


#### Naive Bayes with K-fold CV on Selected PCA Data
```{r echo=FALSE, label='model-nb-kfold-with-pca-1', cache=TRUE}
# Set up k-fold cross-validation
folds_pca <- createFolds(train_pca_selected$Target, k = kfolds)

AllResults_pca <- list()
AllLabels_pca <- list()

for (k in 1:kfolds) {
  fold_train <- train_pca_selected[folds_pca[[k]], ]
  fold_test <- train_pca_selected[-folds_pca[[k]], ]
  
  # Train Naive Bayes Model
  naive_bayes_model <- naiveBayes(Target ~ ., data = fold_train)
  
  # Predict on Test Data
  nb_Pred <- predict(naive_bayes_model, newdata = fold_test)
  
  #Visualize
  plot(nb_Pred, ylab= "Density", main = "Naive Bayes Plot")
  
  # Ensure predictions and actual labels are factors with the same levels
  nb_Pred <- factor(nb_Pred, levels = unique(c(levels(nb_Pred), levels(fold_test$Target))))
  fold_test$Target <- factor(fold_test$Target, levels = unique(c(levels(nb_Pred), levels(fold_test$Target))))
  
  # Visualize Naive Bayes Predictions
  plot <- ggplot(data.frame(Prediction = nb_Pred), aes(x = Prediction)) +
    geom_density(fill = "blue", alpha = 0.5) +
    labs(title = paste("Naive Bayes Plot - Fold", k), y = "Density") +
    theme_minimal()
  print(plot)
  
  
  # Evaluate Model
  conf_matrix <- confusionMatrix(nb_Pred, fold_test$Target)
  print(conf_matrix)
  
  # Calculate accuracy
  accuracy <- sum(diag(conf_matrix$table)) / sum(conf_matrix$table)
  print(paste("Accuracy: Fold", k, "-", round(accuracy * 100, 2), "%", sep = " "))
  
  # Accumulate results from each fold
  AllResults_pca <- c(AllResults_pca, nb_Pred)
  AllLabels_pca <- c(AllLabels_pca, fold_test$Target)
}

# Confusion Matrix (across all folds)
final_conf_matrix_pca <- table(unlist(AllResults_pca), unlist(AllLabels_pca))
print(final_conf_matrix_pca)

# Calculate accuracy from the confusion matrix
nb_accuracy_pca <- sum(diag(final_conf_matrix_pca)) / sum(final_conf_matrix_pca)
print(paste("Overall Accuracy (PCA): ", round(nb_accuracy_pca * 100, 2), "%", sep = ""))

# Plot Confusion Matrix as Heatmap
heatmap_data_pca <- as.matrix(final_conf_matrix_pca)
pheatmap(heatmap_data_pca, main = "Confusion Matrix Heatmap (PCA)", color = colorRampPalette(c("white", "blue"))(50))

# Plotting ROC Curve
roc_curve_pca <- roc(unlist(AllLabels_pca), as.numeric(unlist(AllResults_pca)))
ggroc(roc_curve_pca) +
  ggtitle("ROC Curve (PCA)") +
  xlab("1 - Specificity") +
  ylab("Sensitivity")



```



#### Naive Bayes with K-fold CV on PCA data

```{r echo=FALSE, label='model-nb-kfold-with-pca-2', cache=TRUE}
# Set up k-fold cross-validation
folds_pca <- createFolds(train_pca$Target, k = kfolds)

AllResults_pca <- list()
AllLabels_pca <- list()

for (k in 1:kfolds) {
  fold_train <- train_pca[folds_pca[[k]], ]
  fold_test <- train_pca[-folds_pca[[k]], ]
  
  # Train Naive Bayes Model
  naive_bayes_model <- naiveBayes(Target ~ ., data = fold_train)
  
  # Predict on Test Data
  nb_Pred <- predict(naive_bayes_model, newdata = fold_test)
  
  #Visualize
  plot(nb_Pred, ylab= "Density", main = "Naive Bayes Plot")
  
  # Ensure predictions and actual labels are factors with the same levels
  nb_Pred <- factor(nb_Pred, levels = unique(c(levels(nb_Pred), levels(fold_test$Target))))
  fold_test$Target <- factor(fold_test$Target, levels = unique(c(levels(nb_Pred), levels(fold_test$Target))))
  
  # Visualize Naive Bayes Predictions
  plot <- ggplot(data.frame(Prediction = nb_Pred), aes(x = Prediction)) +
    geom_density(fill = "blue", alpha = 0.5) +
    labs(title = paste("Naive Bayes Plot - Fold", k), y = "Density") +
    theme_minimal()
  print(plot)
  
  # Evaluate Model
  conf_matrix <- confusionMatrix(nb_Pred, fold_test$Target)
  print(conf_matrix)
  
  # Calculate accuracy
  accuracy <- sum(diag(conf_matrix$table)) / sum(conf_matrix$table)
  print(paste("Accuracy: Fold", k, "-", round(accuracy * 100, 2), "%", sep = " "))
  
  # Accumulate results from each fold
  AllResults_pca <- c(AllResults_pca, nb_Pred)
  AllLabels_pca <- c(AllLabels_pca, fold_test$Target)
}

# Confusion Matrix (across all folds)
final_conf_matrix_pca <- table(unlist(AllResults_pca), unlist(AllLabels_pca))
print(final_conf_matrix_pca)

# Calculate accuracy from the confusion matrix
nb_accuracy_pca <- sum(diag(final_conf_matrix_pca)) / sum(final_conf_matrix_pca)
print(paste("Overall Accuracy (PCA): ", round(nb_accuracy_pca * 100, 2), "%", sep = ""))

# Plot Confusion Matrix as Heatmap
heatmap_data_pca <- as.matrix(final_conf_matrix_pca)
pheatmap(heatmap_data_pca, main = "Confusion Matrix Heatmap (PCA)", color = colorRampPalette(c("white", "blue"))(50))

# Plotting ROC Curve
roc_curve_pca <- roc(unlist(AllLabels_pca), as.numeric(unlist(AllResults_pca)))
ggroc(roc_curve_pca) +
  ggtitle("ROC Curve (PCA)") +
  xlab("1 - Specificity") +
  ylab("Sensitivity")


```



### Ensemble Models

#### Random Forest with k-fold CV on Non-PCA Data

```{r echo=FALSE, label='model-randomforest-kfold-no-pca', cache=TRUE}
# Set up k-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)

# Train the model on non-PCA data
rf_model_non_pca <- train(Target ~ ., data = train, method = "rf", trControl = train_control)

# Predict on the test set
predictions_non_pca <- predict(rf_model_non_pca, newdata = test)

# Confusion matrix and accuracy for non-PCA data
conf_matrix_non_pca <- confusionMatrix(predictions_non_pca, test$Target)
print(conf_matrix_non_pca)

# Calculate and print accuracy for non-PCA data
rf_accuracy_non_pca <- conf_matrix_non_pca$overall['Accuracy']
print(paste("Random Forest Accuracy (Non-PCA):", rf_accuracy_non_pca))


```



#### Random Forest with k-fold CV on PCA Data

```{r echo=FALSE, label='model-randomforest-kfold-pca-1', cache=TRUE}
# Train the model on PCA data
rf_model_pca <- train(Target ~ ., data = train_pca, method = "rf", trControl = train_control)

# Predict on the test set
test_pca <- as.data.frame(predict(pca, newdata = test %>% select(where(is.numeric))))
test_pca$Target <- test$Target
predictions_pca <- predict(rf_model_pca, newdata = test_pca)

# Confusion matrix and accuracy for PCA data
conf_matrix_pca <- confusionMatrix(predictions_pca, test$Target)
print(conf_matrix_pca)

# Calculate and print accuracy for PCA data
rf_accuracy_pca <- conf_matrix_pca$overall['Accuracy']
print(paste("Random Forest Accuracy (PCA):", rf_accuracy_pca))

```



#### Random Forest with K-fold CV on Selected PCA Data

```{r echo=FALSE, label='model-randomforest-kfold-pca-2', cache=TRUE}

# Train the model on selected PCA data
rf_model_pca_selected <- train(Target ~ ., data = train_pca_selected, method = "rf", trControl = train_control)

# Transform the test data using the same PCA transformation
test_pca_selected <- as.data.frame(predict(pca, newdata = test %>% select(where(is.numeric)))[, 1:selected_pcs])
test_pca_selected$Target <- test$Target

# Predict on the test set
predictions_pca_selected <- predict(rf_model_pca_selected, newdata = test_pca_selected)

# Confusion matrix and accuracy for selected PCA data
conf_matrix_pca_selected <- confusionMatrix(predictions_pca_selected, test$Target)
print(conf_matrix_pca_selected)

# Calculate and print accuracy for selected PCA data
rf_accuracy_pca_selected <- conf_matrix_pca_selected$overall['Accuracy']
print(paste("Random Forest Accuracy (Selected PCA):", rf_accuracy_pca_selected))




```



#### Random Forest, Variable Tree Numbers, Non PCA data

```{r echo=FALSE, label='model-randomforest-tree-number-no-pca', cache=TRUE}

# Define a grid of different numbers of trees
tune_grid <- expand.grid(.mtry = c(2, 4, 6, 8, 10))

# Train the model on non-PCA data with different numbers of trees
rf_model_non_pca <- train(Target ~ ., data = train, method = "rf", trControl = train_control, tuneGrid = tune_grid)

# Predict on the test set
predictions_non_pca <- predict(rf_model_non_pca, newdata = test)

# Confusion matrix and accuracy for non-PCA data
conf_matrix_non_pca <- confusionMatrix(predictions_non_pca, test$Target)
print(conf_matrix_non_pca)

# Calculate and print accuracy for non-PCA data
rf_accuracy_non_pca <- conf_matrix_non_pca$overall['Accuracy']
print(paste("Random Forest Accuracy (Non-PCA):", rf_accuracy_non_pca))
```



#### Random Forest, Variable Tree Numbers, Selected PCA data

```{r echo=FALSE, label='model-randomforest-tree-number-pca-1', cache=TRUE}
# Define a grid of different numbers of trees
tune_grid <- expand.grid(.mtry = c(2, 4, 6, 8, 10))

# Train the model on selected PCA data with different numbers of trees
rf_model_pca_selected <- train(Target ~ ., data = train_pca_selected, method = "rf", trControl = train_control, tuneGrid = tune_grid)

# Transform the test data using the same PCA transformation
test_pca_selected <- as.data.frame(predict(pca, newdata = test %>% select(where(is.numeric)))[, 1:selected_pcs])
test_pca_selected$Target <- test$Target

# Predict on the test set
predictions_pca_selected <- predict(rf_model_pca_selected, newdata = test_pca_selected)

# Confusion matrix and accuracy for selected PCA data
conf_matrix_pca_selected <- confusionMatrix(predictions_pca_selected, test$Target)
print(conf_matrix_pca_selected)

# Calculate and print accuracy for selected PCA data
rf_accuracy_pca_selected <- conf_matrix_pca_selected$overall['Accuracy']
print(paste("Random Forest Accuracy (Selected PCA):", rf_accuracy_pca_selected))


```


#### Random Forest, Variiable Tree Numbers, PCA data

```{r cache=TRUE, include=FALSE, label='model-randomforest-tree-number-pca-2'}
# Define a grid of different numbers of trees
tune_grid <- expand.grid(.mtry = c(2, 4, 6, 8, 10))

# Ensure the same PCA transformation is applied
pca <- prcomp(train %>% select(where(is.numeric)), center = TRUE, scale. = TRUE)

# Transform the training data
train_pca <- as.data.frame(predict(pca, newdata = train %>% select(where(is.numeric)))[, 1:selected_pcs])
train_pca$Target <- train$Target

# Train the model on selected PCA data with different numbers of trees
rf_model_pca <- train(Target ~ ., data = train_pca, method = "rf", trControl = train_control, tuneGrid = tune_grid)

# Transform the test data using the same PCA transformation
test_pca <- as.data.frame(predict(pca, newdata = test %>% select(where(is.numeric)))[, 1:selected_pcs])
test_pca$Target <- test$Target
```


```{r echo=FALSE, label='model-randomforest-pca2-predict', cache=TRUE}
# Predict on the test set
predictions_pca <- predict(rf_model_pca, newdata = test_pca)

# Confusion matrix and accuracy for selected PCA data
conf_matrix_pca <- confusionMatrix(predictions_pca, test$Target)
print(conf_matrix_pca)

# Calculate and print accuracy for selected PCA data
rf_accuracy_pca <- conf_matrix_pca$overall['Accuracy']
print(paste("Random Forest Accuracy (PCA):", rf_accuracy_pca))


```

### Lazy Learning Models

#### KNN on Non PCA data
```{r echo=FALSE, label='model-knn-no-pca', cache=TRUE}

# Set seed for reproducibility
set.seed(123)

# Define number of folds
kfolds <- 10

# Create folds
folds <- createFolds(train$Target, k = kfolds)

# Initialize lists to store results
AllResults_knn <- list()
AllLabels_knn <- list()

# Train control for caret
train_control <- trainControl(method = "cv", number = kfolds)

for (k in 1:kfolds) {
  # Split data into training and testing sets
  fold_train <- train[folds[[k]], ]
  fold_test <- train[-folds[[k]], ]
  
  # Train k-NN Model using caret
  knn_model <- train(Target ~ ., data = fold_train, method = "knn", trControl = train_control)
  
  # Make predictions
  knn_predictions <- predict(knn_model, fold_test)
  
  # Ensure predictions and actual labels are factors with the same levels
  knn_predictions <- factor(knn_predictions, levels = unique(c(levels(knn_predictions), levels(fold_test$Target))))
  fold_test$Target <- factor(fold_test$Target, levels = unique(c(levels(knn_predictions), levels(fold_test$Target))))
  
  # Visualize k-NN Predictions
  plot <- ggplot(data.frame(Prediction = knn_predictions), aes(x = Prediction)) +
    geom_density(fill = "green", alpha = 0.5) +
    labs(title = paste("k-NN Plot - Fold", k), y = "Density") +
    theme_minimal()
  print(plot)
  
  # Evaluate Model
  conf_matrix <- confusionMatrix(knn_predictions, fold_test$Target)
  print(conf_matrix)
  
  # Calculate accuracy
  accuracy <- sum(diag(conf_matrix$table)) / sum(conf_matrix$table)
  print(paste("Accuracy: Fold", k, "-", round(accuracy * 100, 2), "%", sep = " "))
  
  # Accumulate results from each fold
  AllResults_knn <- c(AllResults_knn, knn_predictions)
  AllLabels_knn <- c(AllLabels_knn, fold_test$Target)
}

# Calculate overall accuracy
overall_accuracy <- confusionMatrix(factor(unlist(AllResults_knn)), factor(unlist(AllLabels_knn)))$overall['Accuracy']
print(paste("Overall k-NN Accuracy:", overall_accuracy))

# Create the final confusion matrix
final_conf_matrix_knn <- confusionMatrix(factor(unlist(AllResults_knn)), factor(unlist(AllLabels_knn)))

# Plot Confusion Matrix as Heatmap
heatmap_data_knn <- as.matrix(final_conf_matrix_knn$table)
pheatmap(heatmap_data_knn, main = "Confusion Matrix Heatmap (k-NN)", color = colorRampPalette(c("white", "green"))(50))

# Convert the matrix to a format suitable for ggplot
melted_heatmap_data <- melt(heatmap_data_knn)



# Plotting ROC Curve
roc_curve_knn <- roc(unlist(AllLabels_knn), as.numeric(unlist(AllResults_knn)))
ggroc(roc_curve_knn) +
  ggtitle("ROC Curve (k-NN)") +
  xlab("1 - Specificity") +
  ylab("Sensitivity")


```



#### KNN on PCA data

```{r echo=FALSE, label='model-knn-pca', cache=TRUE}

# Set seed for reproducibility
set.seed(123)

# Define number of folds
kfolds <- 10

# Create folds
folds <- createFolds(train_pca$Target, k = kfolds)

# Initialize lists to store results
AllResults_knn <- list()
AllLabels_knn <- list()

# Train control for caret
train_control <- trainControl(method = "cv", number = kfolds)

for (k in 1:kfolds) {
  # Split data into training and testing sets
  fold_train_pca <- train_pca[folds[[k]], ]
  fold_test_pca <- train_pca[-folds[[k]], ]
  
  # Train k-NN Model using caret
  knn_model_pca <- train(Target ~ ., data = fold_train_pca, method = "knn", trControl = train_control)
  
  # Make predictions
  knn_predictions_pca <- predict(knn_model_pca, fold_test_pca)
  
  # Ensure predictions and actual labels are factors with the same levels
  knn_predictions_pca <- factor(knn_predictions_pca, levels = unique(c(levels(knn_predictions_pca), levels(fold_test_pca$Target))))
  fold_test_pca$Target <- factor(fold_test_pca$Target, levels = unique(c(levels(knn_predictions_pca), levels(fold_test_pca$Target))))
  
  # Visualize k-NN Predictions
  plot <- ggplot(data.frame(Prediction = knn_predictions_pca), aes(x = Prediction)) +
    geom_density(fill = "green", alpha = 0.5) +
    labs(title = paste("k-NN Plot - Fold", k), y = "Density") +
    theme_minimal()
  print(plot)
  
  # Evaluate Model
  conf_matrix_pca <- confusionMatrix(knn_predictions_pca, fold_test_pca$Target)
  print(conf_matrix_pca)
  
  # Calculate accuracy
  accuracy <- sum(diag(conf_matrix_pca$table)) / sum(conf_matrix_pca$table)
  print(paste("Accuracy: Fold", k, "-", round(accuracy * 100, 2), "%", sep = " "))
  
  # Accumulate results from each fold
  AllResults_knn_pca <- c(AllResults_knn, knn_predictions_pca)
  AllLabels_knn_pca <- c(AllLabels_knn, fold_test_pca$Target)
}

# Calculate overall accuracy
overall_accuracy <- confusionMatrix(factor(unlist(AllResults_knn_pca)), factor(unlist(AllLabels_knn_pca)))$overall['Accuracy']
print(paste("Overall k-NN Accuracy:", overall_accuracy))

# Create the final confusion matrix
final_conf_matrix_knn_pca <- confusionMatrix(factor(unlist(AllResults_knn_pca)), factor(unlist(AllLabels_knn_pca)))

# Plot Confusion Matrix as Heatmap
heatmap_data_knn_pca <- as.matrix(final_conf_matrix_knn_pca$table)
pheatmap(heatmap_data_knn_pca, main = "Confusion Matrix Heatmap (k-NN) PCA", color = colorRampPalette(c("white", "green"))(50))

# Convert the matrix to a format suitable for ggplot
melted_heatmap_data_pca <- melt(heatmap_data_knn_pca)



# Plotting ROC Curve
roc_curve_knn_pca <- roc(unlist(AllLabels_knn_pca), as.numeric(unlist(AllResults_knn_pca)))
ggroc(roc_curve_knn_pca) +
  ggtitle("ROC Curve (k-NN)") +
  xlab("1 - Specificity") +
  ylab("Sensitivity")
```



### Logistic Regression Models 
#### Multinomial Logistic Regression - All Attributes

Given the outcome variable is a factor with three levels, a multinomial logistic regression model is built using the nnet package to fit the "Target" attribute.  The nnet::multinom() function converts factors to indicator variables internally, avoiding the need to convert factor type attributes to dummy variables before running the model.  In addition, the nnet::multinom() function does not require removing the outcome variable from the test data.


The first multinomial logistic regression model is run on all the data to fit for the outcome "Target" attribute.  

For reference, the values of the Target attributed are ordered as follows:

```{r echo=FALSE, label='view-target-levels', cache=TRUE}
levels(studentSuccessData$Target)
```

The model is created using the nnet::multinom() function and used to generate predictions using the test data.

```{r include=FALSE, label='model-multinom-1', cache=TRUE}
#set seed
set.seed(711)

# Fit a logistic regression model using all attributes
multinom_model_Target_All_Data <- nnet::multinom(Target ~ ., data = train)

# Summarize the model
# output a table of coefficients
summary(multinom_model_Target_All_Data)
```



```{r include=FALSE, label='model-multinom-1-predict', cache=TRUE}
# Predict class probabilities
predicted_probs_log_reg_1 <- stats::predict(multinom_model_Target_All_Data, newdata = test, type = "probs")

# Convert probabilities to predicted classes
predicted_classes_log_reg_1 <- colnames(predicted_probs_log_reg_1)[apply(predicted_probs_log_reg_1, 1, which.max)]
```


The confusion matrix generated from the model:

```{r echo=FALSE, label='model-multinom-1-confusionmatrix', cache=TRUE}
# Confusion matrix
# Compare predicted classes to the actual Target values in the test Data
conf_matrix_log_reg_1 <- confusionMatrix(factor(predicted_classes_log_reg_1, levels = levels(test$Target)), test$Target)
print(conf_matrix_log_reg_1)
```


The accuracy generated from the model:

```{r echo=FALSE, label='model-multinom-1-accuracy', cache=TRUE}
# Accuracy
accuracy <- sum(predicted_classes_log_reg_1 == test$Target) / nrow(test)
print(paste("Accuracy:", round(accuracy, 2)))
```



#### Multinomial Logistic Regression - Significant Attributes

The second multinomial logistic regression model is built to fit the outcome "Target" attribute using predictor attributes present in strong and significant rules found during association rule mining above.

The model is created using the nnet::multinom() function and used to generate predictions using the test data.

```{r include=FALSE, label='model-multinom-2', cache=TRUE}
#set seed
set.seed(711)


# Fit a logistic regression model using statistically significant attributes
multinom_model_Target_focused <- nnet::multinom(Target ~ Previous.qualification + Age.at.enrollment + Application.mode + 
                                             Father.s.qualification + Curricular.units.2nd.sem..grade. + 
                                             Tuition.fees.up.to.date + Mother.s.qualification + 
                                             Mother.s.occupation + Displaced, data = train)

# Summarize the model
# output a table of coefficients
summary(multinom_model_Target_focused)
```



```{r include=FALSE, label='model-multinom-2-subset', cache=TRUE}
#first, prepare the test data subset of the same length as predicted classes
test_multinom_focused <- test %>% dplyr::select(Target, Previous.qualification, Age.at.enrollment, Application.mode, 
                                             Father.s.qualification, Curricular.units.2nd.sem..grade.,  
                                             Tuition.fees.up.to.date, Mother.s.qualification, 
                                             Mother.s.occupation, Displaced)

```



```{r include=FALSE, label='model-multinom-2-predict', cache=TRUE}
# Make predictions
predicted_probs_log_reg_2 <- stats::predict(multinom_model_Target_focused, newdata = test_multinom_focused, type = "probs")
predicted_classes_log_reg_2 <- colnames(predicted_probs_log_reg_2)[apply(predicted_probs_log_reg_2, 1, which.max)]
```


The confusion matrix generated from the model:

```{r echo=FALSE, label='model-multinom-2-confusionmatrix', cache=TRUE}
# Combine predictions with actual outcomes
results <- data.frame(
  actual = test$Target,
  predicted = factor(predicted_classes_log_reg_2, levels = levels(test$Target))
)

# Confusion matrix
conf_matrix_log_reg_2 <- caret::confusionMatrix(results$predicted, results$actual)
conf_matrix_log_reg_2
```


The accuracy generated from the model:

```{r echo=FALSE, label='model-multinom-2-accuracy', cache=TRUE}
# Calculate and print the accuracy
accuracy <- conf_matrix_log_reg_2$overall['Accuracy']
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
```




#### Binary Logistic Regression - all attributes

To run a binary logistic regression, additional data transformation is required.  First, a copy of the dataset is created.  A new binary outcome variable titled "Dropout" is added to the dataset to represent contain the value 1 if Target=Dropout.  This attribute will contain a value of 0 when the Target attribute contains "Graduate" or "Enrolled".  In addition, the Target attribute is removed.

```{r cache=TRUE, include=FALSE, label='model-binom-1-dataprep-1'}
studentSuccessData_regrssion <- studentSuccessData

# Remove rows with any NA values
studentSuccessData_regrssion <- studentSuccessData_regrssion[complete.cases(studentSuccessData_regrssion), ]
```



```{r cache=TRUE, include=FALSE, label='model-binom-1-dataprep-2'}
#1.) add a column to hold Dropout, default to 0
studentSuccessData_regression <- studentSuccessData %>% mutate(Dropout=0)
    
#2.) find indexes of the dropout records
dropoutIndexList <- which(studentSuccessData_regression$Target=="Dropout")
    
#3.) populate Dropout column = 1 at the indices
studentSuccessData_regression$Dropout[dropoutIndexList] <- 1

studentSuccessData_regression$Dropout <- as.factor(studentSuccessData_regression$Dropout)

studentSuccessData_regression <- studentSuccessData_regression %>% dplyr::select(-Target)

```


Next, train and test data is created with a 70/30 split, respectively.

```{r cache=TRUE, include=FALSE, label='model-binom-1-train-test'}
set.seed(711)
  
ind <- sample(c(TRUE, FALSE), size=nrow(studentSuccessData_regression), replace=TRUE, prob=c(0.7, 0.3))

train_regression <- studentSuccessData_regression[ind, ]
test_regression <- studentSuccessData_regression[!ind, ]
```


After data preparation, a binary logistic regression model is created to specifically fit for the outcome attribute "Dropout".  The outcome shows coefficient estimates for predictor variables.  Coefficients having a p-value < 0.05 are statistically significant in the model. 

```{r cache=TRUE, include=FALSE, label='model-binom-1-summary'}
#set seed
set.seed(711)

# Fit the logistic regression model
glm_model_Dropout_All_Data <- stats::glm(Dropout ~ ., family=binomial, data = train_regression)

# Summarize the model
summary(glm_model_Dropout_All_Data)

```

The model did not successfully converge to a solution for the generalized linear model (GLM). The model created aliased coefficients indicating that some predictors are colinear with other predictors, or there are redundant predictors in the model leading to singularities.  To address this, an additional and simplified binomial logistic regression model is needed.  The next section builds and tests the simplified model.



#### Binary Logistic Regression - significant attributes

The second binomial logistic regression model is built to fit the outcome "Dropout" attribute using predictor attributes present in strong and significant rules found during association rule mining above.

The binary logistic regression model is Fit to predict Dropout using focused set of attributes.  The following shows the summary of the model:

```{r cache=TRUE, include=FALSE, label='model-binom-2-summary'}
#set seed
set.seed(711)

# Fit the logistic regression model
glm_model_Dropout_Focused <- stats::glm(Dropout ~ Previous.qualification + Age.at.enrollment + Application.mode + 
                                             Father.s.qualification + Curricular.units.2nd.sem..grade. + 
                                             Tuition.fees.up.to.date + Mother.s.qualification + 
                                             Mother.s.occupation + Displaced, family=binomial, data = train_regression)

# Summarize the model
summary(glm_model_Dropout_Focused)

``` 


Next, predictions are run using the model and the test data.

```{r echo=FALSE, label='model-binom-2-predict', cache=TRUE}
# Generate predicted probabilities
predicted_probs_log_reg_4 <- stats::predict(glm_model_Dropout_Focused, newdata = test_regression, type = "response")

# Convert probabilities to class predictions
predicted_classes_log_reg_4 <- ifelse(predicted_probs_log_reg_4 > 0.5, "1", "0")

```


The confusion matrix for the model:

```{r echo=FALSE, label='model-binom-2-confusionmatrix', cache=TRUE}
# Combine predictions with actual outcomes
results <- data.frame(
  actual = test_regression$Dropout,
  predicted = factor(predicted_classes_log_reg_4, levels = levels(test_regression$Dropout))
)


# Generate the confusion matrix
confusion_matrix_log_reg_4 <- confusionMatrix(results$predicted, results$actual)

# Print the confusion matrix
print(confusion_matrix_log_reg_4)

```


The accuracy generated from the model:

```{r echo=FALSE, label='model-binom-2-accuracy', cache=TRUE}
# Print the accuracy
accuracy <- confusion_matrix_log_reg_4$overall['Accuracy']
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
```
\pagebreak
# Results

## Results Summary Table
```{r cache=TRUE, include=FALSE, label='results-summary-combined-data'}

#create vectors to hold results
results_model_1 <- c(37.73,"Random Guess","Base","")
results_model_2 <- c(51.84,"Most Common Value","BAse","")
results_model_3 <- c(0,"K-means","Clustering","NaN - Failed to Converge.")
results_model_4 <- c(73.82,"Decision Tree","Classification","")
results_model_5 <- c(77.87,"Support Vector Machine - Linear","Classification","")
results_model_6 <- c(74.04,"Support Vector Machine - Polynomial","Classification","")
results_model_7 <- c(60.39,"Naive Bayes, K-Fold, Non-PCA","Classification","Average of 10 folds.")
results_model_8 <- c(65.48,"Naive Bayes, K-Fold, Selected-PCA","Classification","Average of 10 folds.")
results_model_9 <- c(64.66,"Naive Bayes, K-Fold, PCA","Classification","Average of 10 folds.")
results_model_10 <- c(77.49,"Random Forest, K-Fold, Non-PCA","Ensemble","")
results_model_11 <- c(76.44,"Random Forest, K-Fold, PCA","Ensemble","")
results_model_12 <- c(66.62,"Random Forest, K-Fold, Selected PCA","Ensemble","")
results_model_13 <- c(76.74,"Random Forest, Variable Tree Numbers, Non-PCA","Ensemble","")
results_model_14 <- c(66.47,"Random Forest, Variable Tree Numbers, Selected PCA","Ensemble","")
results_model_15 <- c(66.54,"Random Forest, Variable Tree Numbers, PCA","Ensemble","")
results_model_16 <- c(59.15,"KNN, non-PCA","Lazy Learning","Average of 10 folds.")
results_model_17 <- c(65.84,"KNN PCA","Lazy Learning","Average of 10 folds.")
results_model_18 <- c(77.64,"Multinomial Logistic Regression - All Attributes","Logistic Regression","")
results_model_19 <- c(69.62,"Multinomial Logistic Regression - Select Attributes","Logistic Regression","")
results_model_20 <- c(0,"Binomial Logistic Regression - All Attributes","Logistic Regression","NaN - Failed to Converge.")
results_model_21 <- c(85.22,"Binomial Logistic Regression - Select Attributes","Logistic Regression","")

#row bind the vectors into a matrix
results_matrix <- rbind(results_model_1, results_model_2,results_model_3,results_model_4,results_model_5,
                        results_model_6,results_model_7,results_model_8,results_model_9,results_model_10,
                        results_model_11,results_model_12,results_model_13,results_model_14,results_model_15,
                        results_model_16,results_model_17,results_model_18,results_model_19,results_model_20,
                        results_model_21)

#convert matrix to a dataframe
all_results_combined_summary <- as.data.frame(results_matrix, stringsAsFactors = FALSE)

# Assign column names to the dataframe
colnames(all_results_combined_summary) <- c("Accuracy", "Model", "Type", "Comments")

# Convert the "Value" column to numeric
all_results_combined_summary$Accuracy <- as.numeric(all_results_combined_summary$Accuracy)

# Sort the dataframe by the "Value" column
all_results_combined_sorted <- all_results_combined_summary[order(all_results_combined_summary$Accuracy, decreasing = TRUE), ]

# Remove row labels by resetting row names to NULL
rownames(all_results_combined_sorted) <- NULL

# Display the sorted dataframe
head(all_results_combined_sorted)
```

```{r echo=FALSE, cache=TRUE, label='results-summary-combined-showoutput'}
kable(all_results_combined_sorted)
```

\pagebreak
## Association Rule Mining
## Clusting Models
## Classification Models
## Ensemble Models
## Lazy Learning Models
## Logistic Regression Models




\pagebreak
# Conclusions & Key Take-Aways



\pagebreak
# Limitations of the Analysis and/or Available Data




\pagebreak
# Bibliography

Realinho,Valentim, Vieira Martins,Mónica, Machado,Jorge, and Baptista,Luís. (2021). Predict Students' Dropout and Academic Success. UCI Machine Learning Repository. https://doi.org/10.24432/C5MC89.
